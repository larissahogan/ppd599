{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Models\n",
    "\n",
    "Overview of today's topics:\n",
    "\n",
    "  - quick refresher\n",
    "  - spatial fixed effects\n",
    "  - spatial regimes\n",
    "  - spatial lag\n",
    "  - spatial error\n",
    "  - geographically-weighted regression\n",
    "  \n",
    "## 1. Quick refresher\n",
    "\n",
    "### 1.1. Theory and models\n",
    "\n",
    "**Spatial models** are models that include geographic information to account for spatial relationships and processes. They can take on many different forms:\n",
    "\n",
    "  - Spatially-explicit regression models (with [PySAL](https://pysal.org))\n",
    "  - Agent-based models and/or cellular automata (with [Mesa](https://mesa.readthedocs.io/))\n",
    "  - Bayesian spatial models using Markov chain Monte Carlo methods (with [PyMC3](https://docs.pymc.io/))\n",
    "\n",
    "We will focus on spatially-explicit regression models here. Spatially-explicit regression models are a type of **statistical model**: sets of assumptions plus mathematical relationships between variables, producing a formal representation of some theory. We are essentially trying to explain the process underlying the generation of our observed data. **Spatial inference** introduces explicit spatial relationships into the statistical modeling framework, as both theory-driven (e.g., spatial spillovers) and data-driven (e.g., MAUP) issues could otherwise violate modeling assumptions.\n",
    "\n",
    "### 1.2. Statistical inference refresher\n",
    "\n",
    "**Statistical inference** is the process of using a sample to *infer* the characteristics of an underlying population (from which this sample was drawn) through estimation and hypothesis testing. What is the probability distribution (the probabilities of occurrence of different possible outcome values of our response variable)? Contrast this with descriptive statistics, which focus simply on describing the characteristics of the sample itself.\n",
    "\n",
    "Common goals of inferential statistics include:\n",
    "\n",
    "  - parameter estimation and confidence intervals\n",
    "  - hypothesis rejection\n",
    "  - prediction and explanation\n",
    "  - model selection\n",
    "\n",
    "Schools of statistical inference:\n",
    "\n",
    "  - frequentist\n",
    "    - frequentists think of probability as proportion of time some outcome occurs (relative frequency)\n",
    "    - given lots of repeated trials, how likely is the observed outcome?\n",
    "    - concepts: statistical hypothesis testing, *p*-values, confidence intervals\n",
    "  - bayesian\n",
    "    - bayesians think of probability as amount of certainty observer has about an outcome occurring (subjective probability)\n",
    "    - probability as a measure of how much info the observer has about the real world, updated as info changes\n",
    "    - concepts: prior probability, likelihood, bayes' rule, posterior probability\n",
    "    \n",
    "### 1.3. Regression refresher\n",
    "\n",
    "This course presumes you're already comfortable with multiple regression and OLS, as a prerequisite.\n",
    "\n",
    "Regression assumptions:\n",
    "\n",
    "  - an additive, linear relationship between response and predictors\n",
    "  - uncorrelated predictors\n",
    "  - uncorrelated, homoskedastic, normally-distributed errors\n",
    "  \n",
    "Regression topics:\n",
    "\n",
    "  - specification: choosing variables to include and the functional form\n",
    "  - transformation: pre-processing to improve linear fit (log, power, etc) and feature scaling\n",
    "  - estimation: using an algorithm (such as OLS, WLS, MLE, etc) to estimate (aka, fit or train) your model's parameters\n",
    "  - validation and diagnostics: model's goodness of fit ($R^2$), parameters' statistical significance ($t$-test and $p$-values), check errors and assumptions (diagnostic tests, residual plot, Q-Q plot, etc), outlier influence (leverage), robustness checks (alternative specifications)\n",
    "  - resampling: cross-validation (out-of-sample prediction with train/test subsets) and bootstrapping (random subsampling to generate estimates' distribution)\n",
    "  - model selection and regularization: bias-variance tradeoff (over/under-fitting), lasso (L1 regularization), ridge (L2 regularization), hyperparameters\n",
    "  \n",
    "## 2. Setup and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pysal as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tracts\n",
    "tracts_ca = gpd.read_file('../../data/tl_2017_06_tract/').set_index('GEOID')\n",
    "\n",
    "# keep LA, ventura, orange counties only (and drop offshore island tracts)\n",
    "to_drop = ['06037599100', '06037599000', '06111980000', '06111990100', '06111003612']\n",
    "tracts_ca = tracts_ca[tracts_ca['COUNTYFP'].isin(['037', '059', '111'])].drop(index=to_drop)\n",
    "\n",
    "# project tracts\n",
    "crs = '+proj=utm +zone=11 +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "tracts_ca = tracts_ca.to_crs(crs)\n",
    "tracts_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df_census = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df_census.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tract geometries with census variables and create med home value 1000s\n",
    "tracts = tracts_ca.merge(df_census, left_index=True, right_index=True, how='left')\n",
    "tracts['med_home_value_k'] = tracts['med_home_value'] / 1000\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will explore a hedonic model of home prices, using a naively specified model that offers lots of opportunities for critique and enhancement. First let's get our data into the right format for estimating our model on them:\n",
    "  - the **design matrix** is a $n×k$ matrix of $n$ non-null observations on $k$ predictor variables\n",
    "  - the **response vector** is a $n×1$ vector of $n$ non-null observations on the response variable (note that PySAL wants its responses to be matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which variables to use as predictors\n",
    "predictors = ['pct_white', 'pct_built_before_1940', 'med_rooms_per_home', 'pct_bachelors_degree']\n",
    "\n",
    "# choose a response variable and drop any rows in which it is null\n",
    "response = 'med_home_value_k'\n",
    "tracts = tracts.dropna(subset=[response])\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the descriptive stats for your model's variables\n",
    "tracts[[response] + predictors].describe().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls) and response matrix\n",
    "X = tracts[predictors].dropna()\n",
    "Y = tracts.loc[X.index][[response]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate linear regression model with OLS\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         name_x=X.columns.tolist(),\n",
    "                         name_y=response,\n",
    "                         name_ds='tracts')\n",
    "print(ols.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our plain old OLS. Now let's explore different kinds of spatial models.\n",
    "\n",
    "### Types of spatially explicit models:\n",
    "\n",
    "  - **Spatial heterogeneity**: account for systematic differences across space without explicitly modeling interdependency\n",
    "    - *spatial fixed effects*: intercept varies for each spatial group\n",
    "    - *spatial regimes*: intercept and coefficients vary for each spatial group\n",
    "    - *geographically weighted regression*: model local relationships that vary across study area\n",
    "  - **Spatial dependence**: model interdependencies between observations through space\n",
    "    - *spatial lag model*: spatially-lagged endogenous variable added as predictor (because of endogeneity, cannot use OLS to estimate)\n",
    "    - *spatial error model*: spatial effects in error term\n",
    "    - *spatial combo model*: both lag and error\n",
    "    \n",
    "## 3. Spatial fixed effects\n",
    "\n",
    "Intercept varies for each each spatial group. Use dummy variables to represent the groups (counties) into which our observations (tracts) are nested. Uses OLS for estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dummy variable for each county, with 1 if tract is in this county and 0 if not\n",
    "for county in tracts['COUNTYFP'].unique():\n",
    "    new_col = f'dummy_county_{county}'\n",
    "    tracts[new_col] = (tracts['COUNTYFP'] == county).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave out one dummy variable to prevent perfect collinearity\n",
    "# ie, a subset of predictors sums to 1 (which full set of dummies will do)\n",
    "county_dummies = [f'dummy_county_{county}' for county in tracts['COUNTYFP'].unique()]\n",
    "county_dummies = county_dummies[:-1]\n",
    "county_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls) and response matrix\n",
    "X = tracts[predictors + county_dummies].dropna()\n",
    "Y = tracts.loc[X.index][[response]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate linear regression model with spatial fixed effects\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         name_x=X.columns.tolist(),\n",
    "                         name_y=response,\n",
    "                         name_ds='tracts')\n",
    "print(ols.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what happens if you change which county dummy you excluded?\n",
    "# how do the coefficients change? which ones do or do not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial regimes\n",
    "\n",
    "Intercept and coefficients vary for each spatial group (aka, regime). Here, the regimes are our 3 counties. In essence, this generates a separate regression model for each regime. We use OLS for estimation, but you can also combine spatial regimes with spatial autoregressive models (the latter is introduced later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls), response matrix, and regimes vector\n",
    "X = tracts[predictors].dropna()\n",
    "Y = tracts.loc[X.index][[response]]\n",
    "regimes = tracts.loc[X.index]['COUNTYFP']\n",
    "regimes.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate spatial regimes model with OLS\n",
    "olsr = ps.model.spreg.OLS_Regimes(y=Y.values,\n",
    "                                  x=X.values,\n",
    "                                  regimes=regimes.values,\n",
    "                                  name_regimes='county',\n",
    "                                  name_x=X.columns.tolist(),\n",
    "                                  name_y=response,\n",
    "                                  name_ds='tracts')\n",
    "print(olsr.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# read through the model output above which county has the largest magnitude\n",
    "# coefficient on pct_white? how would you interpret that in the real world?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geographically weighted regression\n",
    "\n",
    "The problem with global regression models is that they are essentially spatial averages, obfuscating all the local variation in the process you're exploring. GWR allows us to investigate how model parameters and performance vary across the study area. It calibrates a regression model on each observation's local neighborhood then combines these into a global model for the study area. A user-defined *bandwidth* determines how these local models are calibrated: GWR estimates a model for each observation, using all the other observations weighted by their inverse-distance to that observation. The weighting is determined by fitting a *spatial kernel* to the data parameterized by the bandwidth distance.\n",
    "\n",
    "Accordingly, the combination of bandwidth and kernel affects the smoothing (i.e., over-/under-fitting) of your model. Common kernels include the gaussian and bisquare. Bandwidth can be fixed or adaptive. If fixed, then the same distance is used for weighting across every observation's local neighborhood. However, this can introduce problems if your observations vary in density. Consider tracts: a tract in downtown LA may have 100 other tracts within 20km of it, but a tract in the Antelope Valley may have only 2 or 3 (too few for precise estimation). An adaptive bandwidth instead uses a fixed number of nearest neighbors to adjust the bandwidth distance accordingly: tracts in dense areas get a narrower bandwidth distance and tracts in sparse areas get a wider one. For more on GWR, [this book](https://www.wiley.com/en-us/Geographically+Weighted+Regression%3A+The+Analysis+of+Spatially+Varying+Relationships+-p-9780470855256) offers a good gentle introduction.\n",
    "\n",
    "We need to specify fixed vs adaptive bandwidth ([adaptive](https://en.wikipedia.org/wiki/Variable_kernel_density_estimation)), spatial kernel ([gaussian](https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use)), optimization technique ([golden section](https://en.wikipedia.org/wiki/Golden-section_search)), and a criterion to minimize ([AICc](https://en.wikipedia.org/wiki/Akaike_information_criterion#AICc))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_kernel = False\n",
    "spatial_kernel = 'gaussian'\n",
    "search = 'golden_section'\n",
    "criterion = 'AICc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select an adaptive (NN) bandwidth for our GWR model, given the data\n",
    "centroids = tracts.loc[X.index].centroid\n",
    "coords = list(zip(centroids.x, centroids.y))\n",
    "sel = ps.model.mgwr.sel_bw.Sel_BW(coords=coords,\n",
    "                                  y=Y.values,\n",
    "                                  X_loc=X.values,\n",
    "                                  kernel=spatial_kernel,\n",
    "                                  fixed=fixed_kernel)\n",
    "nn = sel.search(search_method=search, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the selected adaptive bandwidth value?\n",
    "# ie, number of NNs to use to determine locally-varying bandwidth distances\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimate the GWR model parameters\n",
    "# pass fixed=False to treat bw as number of NNs (adaptive kernel)\n",
    "model = ps.model.mgwr.gwr.GWR(coords=coords,\n",
    "                              y=Y.values,\n",
    "                              X=X.values,\n",
    "                              bw=nn,\n",
    "                              kernel=spatial_kernel,\n",
    "                              fixed=fixed_kernel)\n",
    "gwr = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the results\n",
    "gwr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the summary statistics across the local models (at bottom of output) to the global model (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a constant was added, so we'll add it to our predictors\n",
    "cols = ['constant'] + predictors\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local parameter estimates into a GeoDataFrame with tract geometries\n",
    "params = pd.DataFrame(gwr.params, columns=cols, index=X.index)\n",
    "params = tracts[['geometry']].merge(params, left_index=True, right_index=True, how='right')\n",
    "params.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to report GWR results is to visualize their spatial distribution.\n",
    "\n",
    "First, we'll create a helper function to generate (properly centered and truncated) colormaps for our subsequent visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate colormaps for GWR plots\n",
    "def get_cmap(values, cmap_name='coolwarm', n=256):\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import LinearSegmentedColormap as lsc\n",
    "    name = f'{cmap_name}_new'\n",
    "    cmap = plt.cm.get_cmap(cmap_name)\n",
    "    vmin = values.min()\n",
    "    vmax = values.max()\n",
    "\n",
    "    if vmax < 0:\n",
    "        # if all values are negative, use the negative half of the colormap\n",
    "        return lsc.from_list(name, cmap(np.linspace(0, 0.5, n)))\n",
    "    elif vmin > 0:\n",
    "        # if all values are positive use the positive half of the colormap\n",
    "        return lsc.from_list(name, cmap(np.linspace(0.5, 1, n)))\n",
    "    else:\n",
    "        # otherwise there are positive and negative values so use zero as midpoint\n",
    "        # and truncate the colormap such that if the original spans ± the greatest\n",
    "        # absolute value, we only use colors from it spanning vmin to vmax\n",
    "        abs_max = max(values.abs())\n",
    "        start = (vmin + abs_max) / (abs_max * 2)\n",
    "        stop = (vmax + abs_max) / (abs_max * 2)\n",
    "        return lsc.from_list(name, cmap(np.linspace(start, stop, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spatial distribution of local parameter estimates\n",
    "# set nrows, ncols to match your number of predictors!\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "for col, ax in zip(predictors, axes.flat):\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Local {col} coefficients')\n",
    "    gdf = params.dropna(subset=[col], axis='rows')\n",
    "    ax = gdf.plot(ax=ax,\n",
    "                  column=col,\n",
    "                  cmap=get_cmap(gdf[col]),\n",
    "                  legend=True,\n",
    "                  legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are our locally-varying parameter estimates. But they're not all statistically significantly different from zero. Where are they (in-)significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local t-values into a GeoDataFrame with tract geometries\n",
    "# set t-values below significance threshold to zero then clip to ± 4\n",
    "# p<.05 corresponds to |t|>1.96, and |t|>4 corresponds to p<.0001\n",
    "tvals = pd.DataFrame(gwr.filter_tvals(alpha=0.05), columns=cols, index=X.index).clip(-4, 4)\n",
    "tvals = tracts[['geometry']].merge(tvals, left_index=True, right_index=True, how='right')\n",
    "\n",
    "# plot the spatial distribution of local t-values\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "for col, ax in zip(predictors, axes.flat):\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Local {col} $t$ values')\n",
    "    gdf = tvals.dropna(subset=[col], axis='rows')\n",
    "    ax = gdf.plot(ax=ax,\n",
    "                  column=col,\n",
    "                  cmap=get_cmap(gdf[col]),\n",
    "                  legend=True,\n",
    "                  legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model perform across the study area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local R-squared values into a GeoDataFrame with tract geometries\n",
    "col = 'Local $R^2$ values'\n",
    "r_squared = pd.DataFrame(gwr.localR2, index=X.index, columns=[col])\n",
    "r_squared = tracts[['geometry']].merge(r_squared, left_index=True, right_index=True, how='right')\n",
    "\n",
    "# plot the spatial distribution of local R-squared values\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title(col)\n",
    "gdf = r_squared.dropna(subset=[col], axis='rows')\n",
    "ax = gdf.plot(ax=ax,\n",
    "              column=col,\n",
    "              cmap='Reds',\n",
    "              legend=True,\n",
    "              legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try increasing or decreasing the nearest neighbors bandwidth value above\n",
    "# how does that change the model's results and visualizations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial diagnostics\n",
    "\n",
    "So far we've seen different spatial heterogeneity models. Now we'll explore spatial dependence (modeling interdependencies between observations over space), starting by using queen-contiguity spatial weights to model spatial relationships between observations and OLS to check diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial weights for only those tracts that appear in design matrix\n",
    "W = ps.lib.weights.Queen.from_dataframe(tracts.loc[X.index])\n",
    "W.transform = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OLS spatial diagnostics to check the nature of spatial dependence\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         w=W,\n",
    "                         spat_diag=True,\n",
    "                         moran=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate moran's I (for the response) and its significance\n",
    "mi = ps.explore.esda.Moran(y=Y, w=W, two_tailed=True)\n",
    "print(mi.I)\n",
    "print(mi.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran's I (for the residuals): moran's i, standardized i, p-value\n",
    "ols.moran_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results: a significant Moran's *I* suggests spatial autocorrelation, but doesn't tell us which alternative specification should be used. Lagrange Multiplier (LM) diagnostics can help with that. If one LM test is significant and the other isn't, then that tells us which model specification (spatial lag vs spatial error) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.lm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial error model: stat, p\n",
    "ols.lm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results: if (and only if) both the LM tests produce significant statistics, try the robust versions (the nonrobust LM tests are sensitive to each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.rlm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial error model: stat, p\n",
    "ols.rlm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... which model specification to choose? Workflow:\n",
    "\n",
    "  1. If neither LM test is significant: use regular OLS.\n",
    "  2. If only one LM test is significant: use that model spec.\n",
    "  3. If both LM tests are significant: run robust versions.\n",
    "  4. If only one robust LM test is significant: use that model spec.\n",
    "  5. If both robust LM tests are significant (this can often happen with large sample sizes):\n",
    "     1. first consider if the initial model specification is actually a good fit\n",
    "     2. if so, use the spatial specification corresponding to the larger robust-LM statistic\n",
    "     3. or consider a combo model\n",
    "\n",
    "A hint for our working example here: our model is *not* well-specified!\n",
    "\n",
    "## 7. Spatial lag model\n",
    "\n",
    "When the diagnostics indicate the presence of a spatial diffusion process. Uses the spatially-lagged endogenous variable as a predictor. Because of endogeneity, cannot use OLS to estimate.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + \\beta X + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ vector of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mll = ps.model.spreg.ML_Lag(y=Y.values,\n",
    "                            x=X.values,\n",
    "                            w=W,\n",
    "                            method='full',\n",
    "                            name_w='queen',\n",
    "                            name_x=X.columns.tolist(),\n",
    "                            name_y=response,\n",
    "                            name_ds='tracts')\n",
    "print(mll.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, rho\n",
    "mll.rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, from my assigned [JAPA article](https://osf.io/t9um6), that the interpretation of spatial-lag models is tricky:\n",
    "\n",
    "> Due to spatial spillover, each coefficient alone does not represent the marginal effect on the response of a unit increase in the predictor. Instead, it represents the direct effect: what happens locally if you make a unit change in the predictor only in one tract. But also present are indirect effects: local spillovers in each tract from a unit predictor change in other tracts.\n",
    "\n",
    "Refer to the article for details on how to calculate and interpret total effects.\n",
    "\n",
    "## 8. Spatial error model\n",
    "\n",
    "When the diagnostics indicate the presence of spatial error dependence (spatial effects in error term).\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\beta X + u$\n",
    "\n",
    "where $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of spatially autocorrelated errors. The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of uncorrelated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mle = ps.model.spreg.ML_Error(y=Y.values,\n",
    "                              x=X.values,\n",
    "                              w=W,\n",
    "                              method='full',\n",
    "                              name_w='queen',\n",
    "                              name_x=X.columns.tolist(),\n",
    "                              name_y=response,\n",
    "                              name_ds='tracts')\n",
    "print(mle.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, lambda\n",
    "mle.lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# re-calculate the spatial weights matrix using distance bands and linear decay\n",
    "# how does that change the diagnostics, lag model, and error model results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Spatial lag+error combo model\n",
    "\n",
    "Estimated with GMM (generalized method of moments). Essentially a spatial error model with endogenous explanatory variables.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + \\beta X + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ vector of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of spatially autocorrelated errors.\n",
    "\n",
    "The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of uncorrelated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmc = ps.model.spreg.GM_Combo_Het(y=Y.values,\n",
    "                                  x=X.values,\n",
    "                                  w=W,\n",
    "                                  name_w='queen',\n",
    "                                  name_ds='tracts',\n",
    "                                  name_x=X.columns.tolist(),\n",
    "                                  name_y=response)\n",
    "print(gmc.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "This has been a tour of different spatially-explicit modeling methods. Your results will only be as good as your initial model's specification! Are the OLS assumptions met? Did we choose theoretically sound predictors that are relatively uncorrelated with each other? Are our variables properly transformed for the best linear fit? In our working example today: no!\n",
    "\n",
    "As a **self-paced exercise**, try including better predictors in the model. Transform them for a better linear fit with the response. Then see how this affects our spatial model results and interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
