{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "encouraging-regard",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "Overview of today's topics:\n",
    "  - model training, evaluation, and tuning\n",
    "  - binomial and multinomial logistic regression\n",
    "  - decision trees and random forests\n",
    "  - *k*-nearest neighbors\n",
    "  - naive bayes\n",
    "  - perceptrons, support vector machines, and the kernel trick\n",
    "  \n",
    "In machine learning, we feed data to an algorithm so that it can make predictions and extract information. Machine learning's broad categories:\n",
    "  - supervised learning: train a model on observed (labeled) data to predict unobserved data\n",
    "    - classification: predict categorical variable\n",
    "    - regresssion: predict continuous variable\n",
    "  - unsupervised learning: discover structure in and extract information from unlabeled data\n",
    "    - clustering: assign observations to groups based on their features\n",
    "    - dimensionality reduction: transform many features to a lower-dimension space\n",
    "  - reinforcement learning: train model by rewarding it when it takes correct action\n",
    "  - artificial neural networks and deep learning\n",
    "    \n",
    "Basic machine learning tasks:\n",
    "  - data collection and cleaning\n",
    "  - feature selection: select a relevant subset of features to train your model\n",
    "  - feature extraction: apply a function to a feature to create a new feature, dimensionality reduction, scaling\n",
    "  - model choice: [identify](https://scikit-learn.org/stable/tutorial/machine_learning_map/) the right  learning algorithm for the task\n",
    "  - model training: train the model on a set of data (if the model is parametric, we often call this step \"parameter estimation\")\n",
    "  - model evaluation: assess its performance on a set of testing data, did it over or underfit?\n",
    "  - model selection and hyperparameter tuning: adjust the features and the hyperparameters the algorithm uses to fit the model for optimum performance\n",
    "  - prediction: use the model to make predictions on unseen data\n",
    "\n",
    "### Probability refresher\n",
    "\n",
    "**Probability** is the ratio of an event occuring to all possible events occurring, whereas the **odds** are the ratio of an event occuring to it not occurring. That is, the odds are the ratio of the probability of an event occurring to the probability of it not occurring: $\\text{odds}=\\frac{p}{1-p}$ and conversely $p=\\frac{\\text{odds}}{1 + \\text{odds}}$\n",
    "\n",
    "For example, if there are 8 blue marbles and 2 red marbles in an urn, the probability of drawing a blue marble is $\\frac{8}{8+2}=0.8$, its odds are $\\frac{8}{2}=4$ (often expressed 4:1) which is equivalent to $\\frac{0.8}{1-0.8}=4$, and its log-odds are therefore $\\log(4)=1.386$.\n",
    "\n",
    "**Log-odds** (the logarithm of the odds) are useful because they take odds that are asymmetrically distributed around 1 and transform them symmetrically around 0, such that $\\log(4)=-\\log(\\frac{1}{4})$, and allow us to linearly combine **odds ratios** by simply adding and subtracting (because the log of a ratio is the log of the numerator minus the log of the denominator). In other words, the *odds* are the ratio of two probabilities, and an *odds ratio* is the ratio of two odds: useful when comparing the odds of a \"what if\" scenario to the odds of the base scenario, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-figure",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "Logistic regression is a regression analysis technique used when the response is binary. Logistic regression is a classification method, but it uses the general linear model and the same formula as linear regression, and it generates a continuous probability value before converting it to a classification prediction. Logistic regression uses maximum likelihood estimation (with regularization by default in scikit-learn) to estimate the parameters of a logit model. It maximizes the (log) [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) function, equivalent to minimizing the cost function via gradient descent.\n",
    "\n",
    "The **logit** model of some probability $p$ represents its log-odds:\n",
    "\n",
    "$\\text{logit}(p) = \\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k$\n",
    "\n",
    "The logit function is the inverse of the logistic function. It takes a value $p$ that ranges from 0 to 1 and converts it to a value that ranges from $-\\infty$ to $+\\infty$, which is necessary for regression analysis. In our example, $p$ represents the probability of being assigned to one of the classes in our classification scheme.\n",
    "\n",
    "**Today we will build some simple models to predict tract poverty status.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify tracts into high poverty vs not\n",
    "df['poverty'] = (df['pct_below_poverty'] > 20).astype(int)\n",
    "df['poverty'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection: which features are important for predicting our categories?\n",
    "response = 'poverty'\n",
    "predictors = ['median_age', 'pct_renting', 'pct_bachelors_degree', 'pct_english_only']\n",
    "data = df[[response] + predictors].dropna()\n",
    "y = data[response]\n",
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling: important for optimal performance especially if algorithm\n",
    "# uses gradient descent or requires regularization\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 70/30 training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "blr = LogisticRegression()\n",
    "y_pred = blr.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the probabilities\n",
    "probs = blr.predict_proba(X_test)\n",
    "df_probs = pd.DataFrame(probs, columns=blr.classes_)\n",
    "df_probs['pred'] = y_pred\n",
    "df_probs['actual'] = y_test.values\n",
    "df_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-rhythm",
   "metadata": {},
   "source": [
    "Manually calculate the probability of observation $i$ belonging to class $1$ as $p = \\frac{e^{\\beta X_i}}{1 + e^{\\beta X_i}}$ where the decision function is $\\text{logit}(p) = \\beta X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the logit (log-odds) for observation 0 and convert to probability\n",
    "# this is its probability of being assigned to class 1\n",
    "log_odds = np.dot(blr.coef_, X_test[0]) + blr.intercept_\n",
    "odds = np.exp(log_odds)\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what is the predicted probability of test case 9 being a high-poverty tract?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-looking",
   "metadata": {},
   "source": [
    "## 2. Classification into multiple categories\n",
    "\n",
    "Binomial logistic regression's predictions assign observations to one of two classes, but many real-world scenarios require three or more classes. The rest of today's examples will explore multi-class supervised learning classification by categorizing tracts into \"low\", \"mid\", or \"high\" poverty status. We will pretend like these class labels are not ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a poverty classification variable\n",
    "# by default, set all as mid poverty tracts\n",
    "df['poverty'] = 'mid'\n",
    "\n",
    "# identify all low poverty tracts\n",
    "mask_low = df['pct_below_poverty'] <= 5\n",
    "df.loc[mask_low, 'poverty'] = 'low'\n",
    "\n",
    "# identify all high poverty tracts\n",
    "mask_high = df['pct_below_poverty'] >= 25\n",
    "df.loc[mask_high, 'poverty'] = 'high'\n",
    "\n",
    "df['poverty'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "response = 'poverty'\n",
    "predictors = ['median_age', 'pct_renting', 'pct_bachelors_degree', 'pct_english_only']\n",
    "data = df[[response] + predictors].dropna()\n",
    "y = data[response]\n",
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-canada",
   "metadata": {},
   "source": [
    "## 3. Multinomial Logistic Regression\n",
    "\n",
    "Multinomial logistic regression generalizes binomial logistic regression to multiple classes. That is, it is a regression analysis technique used when the response is categorical and contains >2 classes. Multinomial logistic regression uses the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to generalize the logistic function to multiple inputs: in probability theory, the softmax represents a probability distribution across a set of possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "mlr = LogisticRegression(multi_class='multinomial', C=1)\n",
    "y_pred = mlr.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-belgium",
   "metadata": {},
   "source": [
    "### Making sense of the probabilities\n",
    "\n",
    "Let's inspect the estimated probabilities of observation $i$ belonging to class $c$ given $\\beta$ and $X_i$, the estimated coefficents and $i$'s features.\n",
    "\n",
    "Then, manually calculate the logit, normalize via softmax, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mlr.predict_proba(X_test)\n",
    "df_probs = pd.DataFrame(probs, columns=mlr.classes_)\n",
    "df_probs['pred'] = y_pred\n",
    "df_probs['actual'] = y_test.values\n",
    "df_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the logit (log-odds) then normalize with softmax function\n",
    "i = 0  # pick an observation\n",
    "log_odds = np.dot(mlr.coef_, X_test[i]) + mlr.intercept_\n",
    "prob = np.exp(log_odds) / np.exp(log_odds).sum()\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-stephen",
   "metadata": {},
   "source": [
    "### Making sense of the coefficients\n",
    "\n",
    "Logistic regression is a parametric method. We have estimated the parameters (coefficients) of a logit model. scikit-learn is a machine learning package and treats logistic regression in the predictive ML paradigm: for a traditional statistical inference treatment of logistic regression, use the statsmodels package instead.\n",
    "\n",
    "Each estimated coefficient is the log of the odds ratio (pay attention to the difference between \"log odds\" and \"log [of the] odds ratio\"). An **odds ratio** is the ratio of the odds of the event occurring to the odds of it not occurring. In our case the event is a 1-unit increase in the predictor. Thus the logit coefficient $\\beta_{c,k}=\\log\\frac{\\text{odds}(y=c | X_k+1)}{\\text{odds}(y=c | X_k)}$ is the ceteris paribus log of the odds of an observation being in class $c$ if $x_k$ is incremented by $1$ divided by the odds of it being in class $c$ if nothing changes. Conversely, the odds ratio is the exponentiated logit coefficient: $\\text{odds ratio} = \\frac{\\text{odds}(y=c | X_k+1)}{\\text{odds}(y=c | X_k)} = e^{\\beta_{c,k}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated coefficients on each variable, for each class\n",
    "df_coeffs = pd.DataFrame(mlr.coef_, columns=X.columns, index=mlr.classes_)\n",
    "df_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the odds ratio for some class and some predictor\n",
    "# a 1-unit increase in predictor k increases the odds of class c by what %\n",
    "B_ck = df_coeffs.loc['low', 'pct_english_only']\n",
    "odds_ratio = np.exp(B_ck)\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-johns",
   "metadata": {},
   "source": [
    "Given an odds ratio $\\rho$, the percent change $\\delta$ in the odds can be calculated as $\\delta = 100(\\rho - 1)$\n",
    "\n",
    "That is, a 1-unit increase in the percent that speak English-only at home is associated with a $\\delta$% increase in the odds of being classified in the low poverty category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually calculate the odds ratio for some observation, class, and predictor\n",
    "i = 0  # observation in position 0 (you can pick any one)\n",
    "c = 1  # class in position 1 (ie, \"low\")\n",
    "k = 3  # predictor in position 3 (ie, \"pct_english_only\")\n",
    "\n",
    "# calculate the logit of class c if nothing changes, then convert to odds\n",
    "x0 = X_test[i]\n",
    "log_odds0 = np.dot(mlr.coef_, X_test[i]) + mlr.intercept_\n",
    "odds0 = np.exp(log_odds0[c]) # convert log-odds to odds\n",
    "\n",
    "# calculate the logit of class c if we increase k by 1, then convert to odds\n",
    "x1 = x0.copy()\n",
    "x1[k] = x1[k] + 1\n",
    "log_odds1 = np.dot(mlr.coef_, x1) + mlr.intercept_\n",
    "odds1 = np.exp(log_odds1[c]) # convert log-odds to odds\n",
    "\n",
    "# calculate the odds ratio\n",
    "odds_ratio = odds1 / odds0\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-level",
   "metadata": {},
   "source": [
    "## 4. Model Assessment\n",
    "\n",
    "The model's performance can be assessed via several **validation** methods, including:\n",
    "\n",
    "  - holdout method: fit model to one subset of data, then test it on a different subset\n",
    "  - *k*-fold cross validation: divide data into *k* groups then, for each group, train the model on all the *other* groups, then test the model on the group and record its assessment score\n",
    "  - bootstrapping: sample with replacement from dataset to assess accuracy\n",
    "  \n",
    "Typical assessments to report include:\n",
    "  - misclassification error rate or, alternatively, accuracy\n",
    "  - precision: what share of all true + false positives are true positives? That is, among everything predicted to be in this class, how many were right?\n",
    "  - recall, aka sensitivity = true positive rate: what share of all true positives + false negatives are true positives? That is, among all the actual items in this class, how many were predicted correctly?\n",
    "  - specificity = true negative rate: what share of all true negatives + false positives are true negatives?\n",
    "  - $F_1$ score: an overall measure of accuracy: the harmonic mean of precision and recall\n",
    "  - plot ROC curves: true positives vs false positives, and measure area under curve\n",
    "  \n",
    "**Bias-variance** tradeoff: you want a model that both 1) captures the nuanced patterns of the training data and 2) generalizes well to new data. However you cannot improve both at the same time, you must trade them off. Overfitting means high variance: your model is too sensitive to noise in the training data and may need regularization, which reduces model complexity. Underfitting means high bias: your model is too smooth and misses important details in the training data. Judicious feature selection, dimensionality reduction, and larger training sample sizes can reduce variance (overfitting). Adding additional predictors can reduce bias (underfitting).\n",
    "\n",
    "For example, with logistic regression, you can adjust regularization with the *C* parameter, which is the inverse of L2 regularization/shrinkage (i.e., lower *C* values give you higher regularization). If your model need not be specified according to specific theory, but rather just needs the greatest predictive accuracy, consider using something like stepwise selection for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate misclassification error rate and accuracy\n",
    "misclassified = (y_test != y_pred).sum()\n",
    "error_rate = misclassified / len(y_test)\n",
    "accuracy = 1 - error_rate\n",
    "error_rate, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "# the report tells about the quality of its predictions\n",
    "# support means how many of each class it saw\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# adjust the C parameter of the logistic regression model\n",
    "# how does it impact our model's accuracy? why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to visualize the model's decision surface\n",
    "# fits model pairwise to just 2 features at a time and plots them\n",
    "def plot_decision(X, y, feature_names, classifier):\n",
    "    \n",
    "    class_colors = {'high': 'r', 'mid': 'y', 'low': 'b'}\n",
    "    class_ints = {'high': 0, 'mid': 1, 'low': 2}\n",
    "    pairs = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "    for ax, pair in zip(axes.flat, pairs):\n",
    "        \n",
    "        # take the two corresponding features\n",
    "        Xp = X[:, pair]\n",
    "        x_min, x_max = Xp[:, 0].min() - 1, Xp[:, 0].max() + 1\n",
    "        y_min, y_max = Xp[:, 1].min() - 1, Xp[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                             np.arange(y_min, y_max, 0.02))\n",
    "        \n",
    "        # fit model to the two features, predict for meshgrid points, then plot\n",
    "        Z = classifier.fit(Xp, y).predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        for cat, i in class_ints.items():\n",
    "            Z[np.where(Z==cat)] = i\n",
    "        cs = ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    \n",
    "        # scatter plot each class in same color as corresponding contour\n",
    "        for cat, color in class_colors.items():\n",
    "            idx = np.where(y == cat)\n",
    "            ax.scatter(Xp[idx, 0], Xp[idx, 1], c=color, label=cat, s=1)\n",
    "\n",
    "        ax.set_xlabel(feature_names[pair[0]])\n",
    "        ax.set_ylabel(feature_names[pair[1]])\n",
    "        ax.figure.tight_layout()        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "# fits model pairwise to just 2 features at a time and plots them\n",
    "plot_decision(X_train, y_train, X.columns, mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-tourism",
   "metadata": {},
   "source": [
    "Look at the figure above. Are our classes linearly separable? What are the implications?\n",
    "\n",
    "**Next steps**: to select the best model for your task, you should choose several algorithms and compare their performance against each other, evaluating and tuning them iteratively. Consider using a [hyperparameter optimization](https://scikit-learn.org/stable/modules/grid_search.html) technique.\n",
    "\n",
    "  1. choose an appropriate learning algorithm for your task\n",
    "  2. choose a key performance metric to evaluate\n",
    "  3. choose a classification algorithm and an optimization algorithm\n",
    "  4. train the model and evaluate its performance\n",
    "  5. tune its hyperparameters to improve performance then re-assess\n",
    "  \n",
    "The rest of the lecture will investigate other candidate algorithms for our task and consider their strengths and limitations.\n",
    "\n",
    "## 5. Decision Trees and Random Forests\n",
    "\n",
    "The logistic regression models we saw earlier were parametric models, meaning they learn a classification function. A **decision tree** is a nonparametric model that partitions the feature space into boxes. It has a tendency to overfit data, but ensembles can help prevent it from getting stuck in a local minimum. Ensemble techniques, which build many individual models then combine them, can be broadly divided into bagging and boosting methods:\n",
    "\n",
    "  - bagging: train many models on different random bootstrap (ie, with replacement) samples of the data then average them (\"bootstrap aggregating\")\n",
    "  - boosting: build ensemble incrementally by empasizing the previously misclassified data points as you train each new model\n",
    "\n",
    "A **random forest** is an ensemble learning method that constructs multiple decision trees then uses bagging. This helps correct for decision trees' overfitting the training data. Random forests tend to work well out of the box, handle nonlinearity well, and work well in very high dimension spaces. Note we don't have to standardize the data to train a decision tree or random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "# set max_depth\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "y_pred = dt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# don't prune the tree: set max_depth to None. what happens? why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model on training data then use it to make predictions with test data\n",
    "# use 1,000 decision trees and all available CPUs\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "y_pred = rf.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-coast",
   "metadata": {},
   "source": [
    "## 6. *k*-Nearest Neighbors\n",
    "\n",
    "kNN is a nonlinear, nonparametric, lazy-learning model and represents an example of instance-based learning.\n",
    "\n",
    "By \"lazy\" we mean that it does not learn a classification function, but rather just memorizes the entire training dataset to subsequently find nearest neighbors. It can require a lot of memory but works well with a small number of dimensions, though less well with high dimensionality: nearest-neighbor search is hard in high-dimension feature spaces because of the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "y_pred = knn.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-avatar",
   "metadata": {},
   "source": [
    "## 7. Naïve Bayes\n",
    "\n",
    "Naïve Bayes is a high bias/low variance classifier that is less likely to overfit small training datasets than a low bias/high variance classifier is (such as kNN or logistic regression). It is a simple algorithm and converges quickly but strongly assumes independence (hence, naïve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "gnb = GaussianNB(priors=None)\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-cholesterol",
   "metadata": {},
   "source": [
    "## 8. Perceptron\n",
    "\n",
    "A perceptron is a simple linear, binary, parametric classifier. It is a very simple single-layer neural network: too simple to be useful for many real-world tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "ppn = Perceptron(eta0=1)\n",
    "y_pred = ppn.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, ppn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-brazilian",
   "metadata": {},
   "source": [
    "## 9. Support Vector Machines\n",
    "\n",
    "SVMs are models that extend the perceptron to find an optimal hyperplane providing the largest margin (ie, separation) between the classes of training data (the training data are the \"support vectors\"). In other words, SVM finds the hyperplane that maximizes the distance between it and the nearest data point on either side of it. SVMs can classify data linearly/parametrically or, using the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method), nonlinearly/nonparametrically if the classes are not linearly separable.\n",
    "\n",
    "An SVM with a linear kernel is very similar to logistic regression. But they might be a good choice instead of logistic regression if the problem is not linearly separable or has a high-dimensional feature space. Choosing the right kernel can be challenging and the results are not straightforwardly explainable. SVMs can be very inefficient to train, so not a good choice for large training data sets.\n",
    "\n",
    "Tuning the SVM's hyperparameters is critical! Here we fit an untuned model as a quick demo, but you should run something like [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "# train the linear SVM (namely, support vector classification)\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "y_pred = svc.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-gravity",
   "metadata": {},
   "source": [
    "### SVMs for nonlinear classification with a kernel function\n",
    "\n",
    "We can turn a linear SVM model into a nonlinear model by using the *kernel trick* to operate in a higher-dimension feature space. In this example, we will use the radial basis function, aka Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "svc_kt = SVC(kernel='rbf', gamma=0.2, C=1)\n",
    "y_pred = svc_kt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface: this is slow\n",
    "plot_decision(X_train, y_train, X.columns, svc_kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try a polynomial kernel. how does that impact model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-competition",
   "metadata": {},
   "source": [
    "Higher gamma parameter values lead to tighter decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "svc_kt2 = SVC(kernel='rbf', gamma=10, C=1)\n",
    "y_pred = svc_kt2.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface: this is slow\n",
    "plot_decision(X_train, y_train, X.columns, svc_kt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-final",
   "metadata": {},
   "source": [
    "Here we see poor generalization because the model was overfitted with that high gamma value: the training overemphasized small fluctations in the training data.\n",
    "\n",
    "## Self-paced exercise\n",
    "\n",
    "Scroll back up to the steps at the bottom of the \"model assessment\" section. Work through those tasks, considering how to improve your model's performance through better feature selection/extraction, hyperparameter optimization, training, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-bundle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
