{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-oxide",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "Overview of today's topics:\n",
    "  - linear discriminant analysis\n",
    "  - principal component analysis\n",
    "  - k-means clustering\n",
    "  - DBSCAN clustering\n",
    "  - hierarchical clustering\n",
    "  - t-sne projection\n",
    "  \n",
    "In unsupervised learning, we use an algorithm to discover structure in and extract information from data. It generally comprises two broad categories:\n",
    "  - **dimensionality reduction**: transform features to a lower-dimension space\n",
    "  - **clustering**: assign observations to groups based on their features\n",
    "  \n",
    "While supervised learning trains a model to make predictions based on a training data set that we feed it, unsupervised learning discovers relationships and groups automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, r2_score, silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-holmes",
   "metadata": {},
   "source": [
    "## 1. Linear discriminant analysis\n",
    "\n",
    "Dimensionality reduction lets us reduce the number of features (variables) in our data set with minimal loss of information. This data compression is called **feature extraction**. Feature extraction is similar to feature selection in that they both reduce the total number of variables in your analysis. In feature selection, we use domain theory or an algorithm to select important variables for our model. Feature extraction instead projects your features onto a lower-dimension space, creating new features rather than just selecting a subset of existing ones.\n",
    "\n",
    "LDA is *supervised* dimensionality reduction, providing a link between supervised learning and dimensionality reduction. It uses a categorical response and continuous features to identify features that account for the most variance between classes (ie, maximum separability). It can be used as a classifier, similar to what we saw last week, or it can be used for dimensionality reduction by projecting the features in the most discriminative directions.\n",
    "\n",
    "We will predict which county a tract is in using 1) a full set of features, and 2) a set of just two projected features. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose response and predictors\n",
    "response = 'county_name'\n",
    "features = ['median_age', 'pct_hispanic', 'pct_white', 'pct_black', 'pct_asian', 'pct_male',\n",
    "            'pct_single_family_home', 'med_home_value', 'med_rooms_per_home', 'pct_built_before_1940',\n",
    "            'pct_renting', 'rental_vacancy_rate', 'avg_renter_household_size', 'med_household_income',\n",
    "            'mean_commute_time', 'pct_commute_drive_alone', 'pct_below_poverty', 'pct_college_grad_student',\n",
    "            'pct_same_residence_year_ago', 'pct_bachelors_degree', 'pct_english_only', 'pct_foreign_born']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = ['Los Angeles', 'Orange', 'Riverside']\n",
    "mask = df['county_name'].isin(counties)\n",
    "subset = features + [response]\n",
    "data = df.loc[mask].dropna(subset=subset)\n",
    "y = data[response]\n",
    "X = data[features]\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data from n dimensions to 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_reduced = lda.fit_transform(X, y)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for county_name in data['county_name'].unique():\n",
    "    mask = y == county_name\n",
    "    ax.scatter(x=X_reduced[mask, 0],\n",
    "               y=X_reduced[mask, 1],\n",
    "               alpha=0.5,\n",
    "               s=3,\n",
    "               label=county_name)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='best', scatterpoints=4)\n",
    "_ = ax.set_title('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-aberdeen",
   "metadata": {},
   "source": [
    "How good are my predictions with just two dimensions? This is a quick and dirty measure of predictive quality between the original, full feature space and the reduced feature space (for a formal analysis, I'd do a test-train split like we saw last week):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate are my predictions using all 22 features?\n",
    "y_pred = LogisticRegression(max_iter=200).fit(X, y).predict(X)\n",
    "print(round(accuracy_score(y, y_pred), 3))\n",
    "\n",
    "# how accurate are my predictions using just 2 projected features?\n",
    "y_pred = LogisticRegression().fit(X_reduced, y).predict(X_reduced)\n",
    "print(round(accuracy_score(y, y_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-answer",
   "metadata": {},
   "source": [
    "We have summarized the most relevant information of our feature space and reduced it from 22 features (ie, dimensions) to just 2. It's not perfect: there has been some information loss, but it's pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try changing the number of counties we retain and the number of dimensions\n",
    "# how does this influence our classification predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-london",
   "metadata": {},
   "source": [
    "## 2. Principal component analysis\n",
    "\n",
    "Remember simple pair-plots? They let you inspect pairwise relationships between your variables. But what if you have lots of features? PCA offers a more rigorous tool. PCA is very similar to exploratory factor analysis, and is often referred to as a type of factor analysis. The former is used to discover relationships in the data, whereas the latter usually implies that you are probing a theoretical (latent) relationship among your variables. We'll focus on PCA today.\n",
    "\n",
    "PCA is used 1) to fix multicollinearity problems and 2) for dimensionality reduction. In the former, it converts a set of original, correlated features into a new set of orthogonal features, which is useful in regression and cluster analysis. In the latter, it summarizes a set of original, correlated features with a smaller number of features that still explain most of the variance in your data (data compression).\n",
    "\n",
    "PCA identifies the combinations of features (directions in feature space) that account for the most variance in the dataset. These orthogonal axes of maximum variance are called principal components. A **principal component** is an eigenvector (direction of maximum variance) of the features' covariance matrix, and the corresponding eigenvalue is its magnitude (factor by which it is \"stretched\"). An eigenvector is the cosine of the angle between a feature and a component. Its corresponding eigenvalue represents the share of variance it accounts for. PCA takes your (standardized) features' covariance matrix, decomposes it into its eigenvectors/eigenvalues, sorts them by eigenvalue magnitude, constructs a projection matrix $W_k$ from the corresponding top $k$ eigenvectors, then transforms the features using the projection matrix to get the new $k$-dimensional feature subspace. Always standardize your data before PCA because it is sensitive to features' scale.\n",
    "\n",
    "We will reduce our feature set to fewer dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is unsupervised, so we don't need a response variable, but we will\n",
    "# define one just so we can build a simple regression model when we're done\n",
    "response = 'med_gross_rent'\n",
    "features = ['median_age', 'pct_hispanic', 'pct_white', 'pct_black', 'pct_asian', 'pct_male',\n",
    "            'pct_single_family_home', 'med_home_value', 'med_rooms_per_home', 'pct_built_before_1940',\n",
    "            'pct_renting', 'rental_vacancy_rate', 'avg_renter_household_size', 'med_household_income',\n",
    "            'mean_commute_time', 'pct_commute_drive_alone', 'pct_below_poverty', 'pct_college_grad_student',\n",
    "            'pct_same_residence_year_ago', 'pct_bachelors_degree', 'pct_english_only', 'pct_foreign_born']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = features + [response]\n",
    "data = df.dropna(subset=subset)\n",
    "y = data[response]\n",
    "X = data[features]\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the features onto all principal components\n",
    "pca = PCA(n_components=None)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our features are correlated with each other, but our principal components are not\n",
    "pd.DataFrame(X_reduced).corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalues represent the variance explained by each component\n",
    "# calculate each component's proportion of variance explained\n",
    "eigenvalues = pca.explained_variance_\n",
    "pve = eigenvalues / eigenvalues.sum()\n",
    "pve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variance-explained plot\n",
    "xpos = range(1, len(features) + 1)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(xpos, pve, marker='o', markersize=5, label='Individual')\n",
    "ax.plot(xpos, np.cumsum(pve), marker='o', markersize=5, label='Cumulative')\n",
    "ax.set_ylabel('Proportion of variance explained')\n",
    "ax.set_xlabel('Principal component')\n",
    "ax.set_xlim(0, len(features) + 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, ls='--')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-announcement",
   "metadata": {},
   "source": [
    "So, how many components should we use? Remember, the goal here is to reduce the dimensionality of the feature set: we want to balance parsimony with explanatory power. There is no single answer, but in general you want the fewest components that explain sufficient variation. So what's the right balance?\n",
    "\n",
    "  - variance-explained criteria: for example, take fewest components necessary to explain, say, 80% of your variance\n",
    "  - visualization criteria: consider that it is impossible to visualize more than 3 dimensions\n",
    "  - elbow criteria: use a scree plot (aka, variance-explained plot) and look for an \"elbow\" in the curve\n",
    "  - kaiser criteria: use components with an eigenvalue >1 (an obsolete method today)\n",
    "  \n",
    "For visualization purposes, let's use two components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the features onto a 2-dimensional subspace\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see our projected features\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-thanksgiving",
   "metadata": {},
   "source": [
    "We often refer to these projected data as \"principal component scores\" or a \"score matrix\", $T_k$, where $T_k = XW_k$ and $X$ is your original feature matrix and $W_k$ is the projection matrix, that is, a matrix containing the first $k$ principal components (ie, the $k$ eigenvectors with the largest corresponding eigenvalues). In our case, $k=2$. We can calculate this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project our features manually onto the two dimensions\n",
    "eigenvectors = pca.components_.T\n",
    "np.dot(X, eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-milwaukee",
   "metadata": {},
   "source": [
    "Loadings represent the correlations between the features and the components. Loadings are the eigenvectors scaled by the square roots of their eigenvalues (aka, \"singular values\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_\n",
    "loadings = eigenvectors * np.sqrt(eigenvalues)\n",
    "\n",
    "# turn into a DataFrame with column names and row labels\n",
    "cols = [f'PC{i}' for i in range(1, pca.n_components_ + 1)]\n",
    "pd.DataFrame(loadings, index=features, columns=cols).sort_values('PC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate are my predictions using all 22 features?\n",
    "y_pred = LinearRegression().fit(X, y).predict(X)\n",
    "print(round(r2_score(y, y_pred), 3))\n",
    "\n",
    "# how accurate are my predictions using just the first 2 principal components?\n",
    "y_pred = LinearRegression().fit(X_reduced, y).predict(X_reduced)\n",
    "print(round(r2_score(y, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the points on their first 2 PCs, and color by the response variable\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax = sns.scatterplot(ax=ax, x=X_reduced[:, 0], y=X_reduced[:, 1],\n",
    "                     hue=y, palette='plasma_r', s=5, edgecolor='none')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "_ = ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# conduct a PCA with the first 4 components\n",
    "# how does our predictive accuracy change? how does the scatterplot change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-franchise",
   "metadata": {},
   "source": [
    "## 3. k-means clustering\n",
    "\n",
    "Dimensionality reduction projects our data onto a lower-dimension space, usually through unsupervised learning. A second branch of unsupervised learning, **cluster analysis**, lets us discover natural groups that exist in our data. Last week we predicted groups in labeled data by training a supervised learning algorithm. In cluster analysis, we discover unknown groups in unlabeled data through an unsupervised learning algorithm. As with dimensionality reduction, remember to standardize your data before clustering. Many clustering algorithms work well in high-dimensional feature spaces, but some work better after PCA dimensionality reduction (due to the curse of dimensionality).\n",
    "\n",
    "**k-means** is probably the most common clustering algorithm. It clusters data into $k$ groups based on their similarity. It is a form of **prototype-based clustering** where each cluster is represented by a prototype, or centroid. You have to specify the number of groups in advance. This works well when you want to partition your data into a predetermined number of groups. Otherwise, you have to determine an optimal value for $k$.\n",
    "\n",
    "Here, we will identify counties that are similar to one another across a wide variety of characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['median_age', 'pct_hispanic', 'pct_white', 'pct_black', 'pct_asian', 'pct_male', 'med_gross_rent',\n",
    "            'pct_single_family_home', 'med_home_value', 'med_rooms_per_home', 'pct_built_before_1940',\n",
    "            'pct_renting', 'rental_vacancy_rate', 'avg_renter_household_size', 'med_household_income',\n",
    "            'mean_commute_time', 'pct_commute_drive_alone', 'pct_below_poverty', 'pct_college_grad_student',\n",
    "            'pct_same_residence_year_ago', 'pct_bachelors_degree', 'pct_english_only', 'pct_foreign_born']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate then standardize median values across counties\n",
    "counties = df.groupby('county_name').median()\n",
    "X = counties[features].dropna()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project onto first two principal components for 2-D clustering\n",
    "X_reduced = PCA(n_components=2).fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the data\n",
    "km = KMeans(n_clusters=5).fit(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cluster labels, the unique labels, and the number of clusters obtained\n",
    "cluster_labels = km.labels_\n",
    "unique_labels = set(cluster_labels)\n",
    "num_clusters = len(unique_labels)\n",
    "print(f'Number of clusters: {num_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cluster_labels).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot points on first two PCs and color by cluster\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax = sns.scatterplot(ax=ax, x=X_reduced[:, 0], y=X_reduced[:, 1],\n",
    "                     hue=cluster_labels, palette='Set1', s=20, edgecolor='none')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "_ = ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score is the average silhouette coefficient\n",
    "silhouette_score(X_reduced, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-realtor",
   "metadata": {},
   "source": [
    "The **silhouette** score measures *cohesion* vs *separation*: how similar the points are to to their own clusters vs to the other clusters, on average. This measures how tightly grouped our clusters are. The silhouette can range from -1 to +1. Negative values suggest clustering problems, including too many/few clusters.\n",
    "\n",
    "So how do you pick a good $k$?\n",
    "  - theoretically, how many clusters should there be in your data (if knowable beforehand)?\n",
    "  - which $k$ value gives you the best silhouette score?\n",
    "  - elbow criteria (similar to what we saw for PCA): find an elbow in the line plot of distortion vs cluster count. Distortion is also called inertia, and represents the sum of squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an elbow plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Distortion')\n",
    "kvals = range(1, 15)\n",
    "distortions = []\n",
    "for k in kvals:\n",
    "    km = KMeans(n_clusters=k).fit(X_reduced)\n",
    "    distortions.append(km.inertia_)\n",
    "ax.plot(kvals, distortions, marker='o')\n",
    "_ = ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# use the elbow plot above to choose a new k value\n",
    "# how does it affect the scatterplot and silhouette score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-trainer",
   "metadata": {},
   "source": [
    "## 4. DBSCAN clustering\n",
    "\n",
    "DBSCAN (density-based spatial clustering of applications with noise) represents another form of clustering known as **density-based clustering**. Density-based clustering works better in low-dimension feature spaces, so PCA in advance is a good idea.\n",
    "\n",
    "DBSCAN assigns cluster labels based on dense regions of points, by identifying core points, border points, and noise points. Unlike k-means, we do not need to know the number of clusters beforehand. We parameterize it with a minimum number of points that must fall within some radius $\\epsilon$ of a point to consider that point a core point. The $\\epsilon$ parameter represents the maximum distance in the feature space that points can be from each other to be considered a cluster. The min_samples parameter is the minimum cluster size allowed: everything else gets classified as noise.\n",
    "\n",
    "DBSCAN can be useful for geospatial clustering of either projected coordinates, or lat-long coordinates if you use a haversine distance metric. But here, we will just cluster our same features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the data (in two dimensions again)\n",
    "X_reduced = PCA(n_components=2).fit_transform(X)\n",
    "db = DBSCAN(eps=1, min_samples=3, metric='euclidean').fit(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cluster labels, the unique labels, and the number of clusters obtained\n",
    "cluster_labels = db.labels_\n",
    "unique_labels = set(cluster_labels)\n",
    "num_clusters = len(unique_labels)\n",
    "print(f'Number of clusters: {num_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot points on first two PCs and color by cluster\n",
    "# cluster label -1 means noise\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax = sns.scatterplot(ax=ax, x=X_reduced[:, 0], y=X_reduced[:, 1],\n",
    "                     hue=cluster_labels, palette='Set1', s=20, edgecolor='none')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "_ = ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X_reduced, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try changing the epsilon and min_samples then re-clustering\n",
    "# how does it change the silhouette score and the cluster plot?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-unemployment",
   "metadata": {},
   "source": [
    "## 5. Hierarchical clustering\n",
    "\n",
    "Another form of clustering is **hierarchical clustering**, which can be agglomerative or divisive. Agglomerative clustering initially treats each observation as its own cluster, then iteratively merges the closest two clusters until only one supercluster remains. There are four common algorithms:\n",
    "  - single linkage: calculate distance between the most similar members in each pair of clusters, then merge the two clusters with smallest such distance\n",
    "  - complete linkage: like single linkage, but instead compare the most dissimilar members\n",
    "  - average linkage: calculate average distance between all members in each pair of clusters, then merge the two clusters with smallest average distance\n",
    "  - Ward's linkage: merge the two clusters that cause the least increase in total within-cluster sum of squared errors\n",
    "  \n",
    "We could use scikit-learn, but I prefer agglomerative clustering in scipy so we can easily visualize the dendrogram. A dendrogram shows us how the clusters link up and lets us explore which observations are more/less similar. The dendrogram's structure suggests high-level superclusters and we can cut its tree at an arbitrary level.\n",
    "\n",
    "In this example, we'll cluster in four dimensions, which was suggested by our PCA variance-explained plot earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project onto first 4 principal components\n",
    "X_reduced = PCA(n_components=4).fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance matrix then linkage matrix, choosing a method (algorithm)\n",
    "distances = pdist(X_reduced)\n",
    "Z = hierarchy.linkage(distances, method='complete', optimal_ordering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cophenetic correlation measures how well clustering preserved pairwise distances\n",
    "c, _ = hierarchy.cophenet(Z, distances)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a distance to cut dendrogram tree\n",
    "cut_point = 6\n",
    "\n",
    "# plot the dendrogram, colored by clusters below the cut point\n",
    "fig, ax = plt.subplots(figsize=(5, 11))\n",
    "ax.set_xlabel('Euclidean distance')\n",
    "with plt.rc_context({'lines.linewidth': 1}):\n",
    "    R = hierarchy.dendrogram(Z=Z,\n",
    "                             orientation='right',\n",
    "                             labels=counties.index,\n",
    "                             color_threshold=cut_point,\n",
    "                             distance_sort='descending',\n",
    "                             show_leaf_counts=False,\n",
    "                             ax=ax)\n",
    "plt.axvline(cut_point, c='k')\n",
    "fig.savefig('dendrogram.png', dpi=600, facecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign k cluster labels to the observations, based on where you cut tree\n",
    "# k = number of clusters = how many horizontal lines you intersected above\n",
    "k = 8\n",
    "cluster_labels = hierarchy.fcluster(Z, t=k, criterion='maxclust')\n",
    "pd.Series(cluster_labels).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# pick different points to cut the tree, how many clusters do they imply?\n",
    "# which cut point is the right one to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-personality",
   "metadata": {},
   "source": [
    "## 6. t-SNE\n",
    "\n",
    "What if I want to discover structure in >3 dimensions (like we did above), but still be able to visualize it?\n",
    "\n",
    "Manifold learning is a nonlinear dimensionality reduction approach that usually uses unsupervised learning. **t-SNE** (t-distributed stochastic neighbor embedding) is a manifold learning technique used for projecting high-dimension data sets into a plane for easy visualization. Here, we project our counties' higher-dimension feature space to 2 dimensions for visualization. t-SNE projection is useful because it preserves group structure relatively well despite information loss. However, given the global density-equalizing nature of t-SNE, relative distances within and between clusters are not preserved and should not be interpreted otherwise. \n",
    "\n",
    "For an example of using clustering + t-SNE to discover and visualize similar places, see this [ANS article](https://doi.org/10.1007/s41109-019-0189-1). Here, we will use t-SNE to project our data to two dimensions to scatterplot the hierarchical clusters from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE with two dimensions, then project features onto this space\n",
    "tsne = TSNE(n_components=2, n_iter=10000, random_state=0)\n",
    "X_reduced = pd.DataFrame(data=tsne.fit_transform(X),\n",
    "                         index=counties.index,\n",
    "                         columns=['TC1', 'TC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the colored clusters projected onto the two t-SNE dimensions\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "X_reduced['color'] = pd.Series(dict(zip(R['ivl'], R['leaves_color_list'])))\n",
    "ax.scatter(x=X_reduced['TC1'], y=X_reduced['TC2'], c=X_reduced['color'], s=10)\n",
    "\n",
    "# identify a county of interest in the plot\n",
    "county = 'San Francisco'\n",
    "_ = ax.scatter(x=X_reduced.loc[county, 'TC1'],\n",
    "               y=X_reduced.loc[county, 'TC2'],\n",
    "               alpha=1, marker='o', s=300, linewidth=2, color='none', ec='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# pick different points to cut the tree, how does it change our t-SNE plot?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
