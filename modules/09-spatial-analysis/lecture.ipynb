{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Analysis\n",
    "\n",
    "\"Everything is related to everything else, but near things are more related than distant things.\" -Waldo Tobler\n",
    "\n",
    "If the strength of a relationship between entities increases with their proximity, then spatial analysis/modeling is essential for understanding the relationship's process and pattern. Today we focus on exploratory spatial data analysis (ESDA) to discover patterns in spatial data.\n",
    "\n",
    "Overview of today's topics:\n",
    "  - Tobler's first law of geography\n",
    "  - spatial weights\n",
    "  - spatial interpolation\n",
    "  - spatial lag\n",
    "  - spatial autocorrelation\n",
    "  - hot spot mapping\n",
    "  \n",
    "Today we will conduct an exploratory spatial analysis of LA county household income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysal as ps\n",
    "import seaborn as sns\n",
    "from scipy.stats import stats\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. California census tract geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tracts, display shape\n",
    "tracts_ca = gpd.read_file('../../data/tl_2017_06_tract/')\n",
    "tracts_ca = tracts_ca.set_index('GEOID')\n",
    "tracts_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what variables are present?\n",
    "tracts_ca.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 5 rows\n",
    "tracts_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain LA county only (and drop channel island tracts)\n",
    "tracts_ca = tracts_ca[tracts_ca['COUNTYFP']=='037'].drop(index=['06037599100', '06037599000'])\n",
    "tracts_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project spatial geometries to a meter-based projection for SoCal\n",
    "crs = '+proj=utm +zone=11 +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "tracts_ca = tracts_ca.to_crs(crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. California tract-level census variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df_census = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df_census.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tract geometries with census variables\n",
    "tracts = tracts_ca.merge(df_census, left_index=True, right_index=True, how='left')\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pop density in persons per sq km\n",
    "# turn any infinities into nulls\n",
    "tracts['pop_density'] = tracts['total_pop'] / (tracts['ALAND'] / 1e6)\n",
    "tracts = tracts.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial exploration\n",
    "\n",
    "Let's do some quick mapping and analysis of distributions and correlations for a couple variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "tracts['med_household_income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "tracts['pop_density'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect these variables' statistical distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 2))\n",
    "ax1 = sns.boxplot(ax=axes[0], x=tracts['med_household_income'])\n",
    "ax2 = sns.boxplot(ax=axes[1], x=tracts['pop_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map a couple variables to inspect their spatial distributions\n",
    "cols = ['pop_density', 'med_household_income']\n",
    "for col in cols:\n",
    "    ax = tracts.dropna(subset=[col]).plot(column=col,\n",
    "                                          scheme='NaturalBreaks',\n",
    "                                          cmap='plasma',\n",
    "                                          figsize=(4, 4),\n",
    "                                          legend=True,\n",
    "                                          legend_kwds={'bbox_to_anchor': (1.7, 1)})\n",
    "    ax.set_title(col)\n",
    "    _ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some missing values. We'll spatially interpolate them later.\n",
    "\n",
    "Visually, it appears that these two variables may be negatively correlated? In general, where one is high, the other is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation coefficient and p-value\n",
    "subset = tracts.dropna(subset=['pop_density', 'med_household_income'])\n",
    "r, p = stats.pearsonr(x=subset['pop_density'],\n",
    "                      y=subset['med_household_income'])\n",
    "print('r={:.4f}, p={:.4f}'.format(r, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty scatter plot with matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=subset['pop_density'],\n",
    "                y=subset['med_household_income'],\n",
    "                s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model with scipy\n",
    "# what if you log transform your variables first?\n",
    "m, b, r, p, se = stats.linregress(x=subset['pop_density'],\n",
    "                                  y=subset['med_household_income'])\n",
    "print(f'm={m:.4f}, b={b:.4f}, r^2={r**2:.4f}, p={p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every 1 person/km^2 increase in density is associated with a *m* change in median household income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# look through the list of columns, pick two new variables, and map them\n",
    "# do they look like they are correlated? would you expect them to be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spatial weights matrix\n",
    "\n",
    "Spatial analysis depends on spatial relationships. A spatial weights matrix defines the spatial relationships among our units of analysis (tracts, in this case). It tells how they're spatially connected to one another. These weights can take on many different forms. Pick the right form for your theoretical needs, including:\n",
    "\n",
    "  - rook contiguity\n",
    "  - queen contiguity\n",
    "  - k-nearest neighbors\n",
    "  - distance band\n",
    "\n",
    "### 3.1. Contiguity-based weights: rook contiguity\n",
    "\n",
    "Using rook contiguity, two spatial units must share an edge of their boundaries to be considered neighbors. This isn't terribly common in practice since queen is usually more useful, but it's worth understanding as a trivial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tract labels (GEOIDs) and pick one (arbitrarily) to work with throughout\n",
    "labels = tracts.index.tolist()\n",
    "label = labels[603]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate rook spatial weights\n",
    "w_rook = ps.lib.weights.Rook.from_dataframe(tracts, ids=labels, id_order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neighbors of some tract\n",
    "# this is a raw contiguity matrix, so weights are binary 1s and 0s meaning neighbor/not\n",
    "w_rook[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Contiguity-based weights: queen contiguity\n",
    "\n",
    "Using queen contiguity, two spatial units need only share a vertex (a single point) of their boundaries to be considered neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate queen spatial weights\n",
    "w_queen = ps.lib.weights.Queen.from_dataframe(tracts, ids=labels, id_order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neighbors of some tract\n",
    "# this is a raw contiguity matrix, so weights are binary 1s and 0s meaning neighbor/not\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many neighbors does this tract have?\n",
    "w_queen.cardinalities[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cardinalities to series and describe data\n",
    "pd.Series(w_queen.cardinalities).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many neighbors does the average tract have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of neighbors\n",
    "w_queen.min_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of neighbors\n",
    "w_queen.max_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# islands are observations with no neighbors, disconnected in space (can cause modeling problems)\n",
    "w_queen.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a census tract of interest, along with its neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tracts.plot(ax=ax, facecolor='#666666', edgecolor='w', linewidth=0.5)\n",
    "\n",
    "# plot some tract of interest in red\n",
    "tract = tracts.loc[[label]]\n",
    "tract.plot(ax=ax, facecolor='#ff0000', edgecolor='w', linewidth=2)\n",
    "\n",
    "# plot the neighbors in blue\n",
    "neighbors = tracts.loc[w_queen[label]]\n",
    "neighbors.plot(ax=ax, facecolor='#0033cc', edgecolor='w', linewidth=2)\n",
    "\n",
    "# zoom to area of interest\n",
    "xmin, ymin, xmax, ymax = neighbors.unary_union.bounds\n",
    "ax.axis('equal')\n",
    "ax.set_xlim(xmin-100, xmax+100)  # +/- 100 meters\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.set_title('Neighbors of tract {}'.format(label))\n",
    "_ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# draw a queen-contiguity graph of the tracts\n",
    "fig, ax = plt.subplots(figsize=(12, 12), facecolor='#111111')\n",
    "tracts.plot(ax=ax, facecolor='#333333', edgecolor='k', linewidth=0.3)\n",
    "\n",
    "# extract centroids of tract and its neighbors, then draw lines between them\n",
    "for tract, neighbors in w_queen:\n",
    "    tract_centroid = tracts.loc[tract, 'geometry'].centroid\n",
    "    for neighbor_centroid in tracts.loc[neighbors, 'geometry'].centroid:\n",
    "        Xs = [tract_centroid.x, neighbor_centroid.x]\n",
    "        Ys = [tract_centroid.y, neighbor_centroid.y]\n",
    "        ax.plot(Xs, Ys, color='r', linewidth=0.3)\n",
    "_ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Distance-based weights: *k*-nn\n",
    "\n",
    "Find the *k*-nearest neighbors of each tract, by centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# k-nearest neighbors finds the closest k tract centroids to each tract centroid\n",
    "w_knn = ps.lib.weights.KNN.from_dataframe(tracts, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they all have exactly k neighbors\n",
    "w_knn.neighbors[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Distance-based weights: distance band\n",
    "\n",
    "Here, other tracts are considered neighbors of some tract if they are within a given threshold distance of it, by centroid. Distance band weights can be specified to take on continuous values rather than binary (1s and 0s), with these values being the inverse distance between each pair of \"neighboring\" units.\n",
    "\n",
    "  - linear distance-decay exponent is -1, so $w_l=\\frac{1}{d}$\n",
    "  - gravity model distance-decay exponent is -2, so $w_g=\\frac{1}{d^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate maximum nearest neighbor distance so each unit is assured of >=1 neighbor\n",
    "x = tracts.centroid.x\n",
    "y = tracts.centroid.y\n",
    "coords = np.array([x, y]).T\n",
    "threshold = ps.lib.weights.min_threshold_distance(coords)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate linear decay continuous weights\n",
    "w_dist = ps.lib.weights.distance.DistanceBand.from_dataframe(tracts,\n",
    "                                                             threshold=threshold,\n",
    "                                                             binary=False,\n",
    "                                                             alpha=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many distance-band neighbors does our tract have?\n",
    "len(w_dist.neighbors[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the neighbors, colored by weight from nearest to furthest\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "tracts.plot(ax=ax, facecolor='#333333', edgecolor='gray', linewidth=0.1)\n",
    "\n",
    "# get the tract of interest and its neighbors/weights\n",
    "tract = tracts.loc[[label]]\n",
    "weights = pd.Series(w_dist[label])\n",
    "neighbors = tracts.loc[weights.index, ['geometry']]\n",
    "neighbors['weights_scaled'] = weights\n",
    "\n",
    "# plot the tract's neighbors in blues by weight\n",
    "neighbors.plot(ax=ax,\n",
    "               column='weights_scaled',\n",
    "               cmap='Blues_r',\n",
    "               edgecolor='gray',\n",
    "               linewidth=0.3,\n",
    "               scheme='NaturalBreaks')\n",
    "\n",
    "# plot the tract of interest in red\n",
    "tract.plot(ax=ax, facecolor='r', edgecolor='r', linewidth=0.1)\n",
    "\n",
    "# zoom to area of interest\n",
    "xmin, ymin, xmax, ymax = neighbors.unary_union.bounds\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title('Neighbors of tract {}'.format(label))\n",
    "_ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# recompute the distance-based spatial weights with a gravity decay\n",
    "# how and why does this impact the number of neighbors and the map above? why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Standardizing weights\n",
    "\n",
    "A spatial weights matrix with raw values (e.g., binary 1s and 0s for neighbor/not) is not always the best for analysis. Some sort of standardization is useful. Typically, we want to apply a row-based transformation so every row of the matrix sums up to 1. We'll see some examples of why this matters in practice shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the neighbors and weights of our tract\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current transformation of the weights matrix (O = original)\n",
    "w_queen.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the queen weights\n",
    "w_queen.set_transform('R')\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the linear-decay distance-based weights\n",
    "w_dist.set_transform('R')\n",
    "#w_dist[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySAL supports the following transformations:\n",
    "\n",
    "  - O: original, returning the object to the initial state\n",
    "  - B: binary, with every neighbor having assigned a weight of 1\n",
    "  - R: row-based, with all the neighbors of a given observation adding up to 1\n",
    "  - V: variance stabilizing, with the sum of all the weights being constrained to the number of observations\n",
    "\n",
    "It can take a long time to calculate a weights matrix for a large data set. Once you've created yours, you might want to save it to disk to re-use in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your matrix to disk\n",
    "f = ps.lib.io.open('tracts_queen.gal', 'w')\n",
    "f.write(w_queen)\n",
    "f.close()\n",
    "\n",
    "# read a matrix from disk (notice its transformation)\n",
    "w_queen = ps.lib.io.open('tracts_queen.gal', 'r').read()\n",
    "w_queen[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial interpolation\n",
    "\n",
    "Interpolation lets you estimate unobserved values based on observed values. With spatial data, you can perform spatial interpolation by filling in missing data points based on nearby values. This assumes positive spatial autocorrelation exists: more on that in a moment. Remember Tobler's first law of geography.\n",
    "  \n",
    "  - **Nearest neighbor** interpolation is perhaps the simplest method: just assign the value of the nearest neighbor\n",
    "  - **Local averaging** assigns missing values by taking the average of adjacent neighbors or neighbors within some radius\n",
    "  - **Inverse distance weighting** assigns missing values using a distance weighted average: that is, the mean weighs nearby values more than it weighs distant values (and your distance decay choice is important!)\n",
    "  - **Kriging** is a sophisticated method that incorporates information about spatial trends and autocorrelation with a variogram\n",
    "\n",
    "We'll look at an example comparing local averaging to inverse distance weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many tracts are missing values for this variable?\n",
    "col = 'med_household_income'\n",
    "nulls = tracts[pd.isnull(tracts[col])].index\n",
    "len(nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, this tract is missing a value\n",
    "tract = nulls[0]\n",
    "tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local averaging: equal-weighted queen-adjacent tracts\n",
    "neighbors = w_queen[tract]\n",
    "tracts.loc[neighbors, col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, calculate inverse distance weighted mean\n",
    "neighbors = w_dist[tract]\n",
    "inv_dist_wt = pd.Series(neighbors)\n",
    "(tracts.loc[neighbors, col] * inv_dist_wt).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, interpolate all the missing values across this variable\n",
    "estimates = {}\n",
    "for tract in nulls:\n",
    "    neighbors = w_dist[tract]\n",
    "    inv_dist_wt = pd.Series(w_dist[tract])\n",
    "    estimates[tract] = (tracts.loc[neighbors, col] * inv_dist_wt).sum()\n",
    "pd.Series(estimates).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# spatially interpolate missing values for median home value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial lag\n",
    "\n",
    "Spatial lag tells us how values locate near other (dis)similar values. While spatial interpolation filled in unobserved (missing) values using nearby values, spatial lag lets us compare observed values to their nearby values.\n",
    "\n",
    "Here we calculate the spatial lag of a variable. If the spatial weights matrix is row-standardized (important), then the spatial lag is the average value of an observation's neighbors, however \"neighbor\" is defined in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a variable to investigate and drop null rows\n",
    "col = 'med_household_income'\n",
    "tracts_not_null = tracts[[col, 'geometry']].dropna()\n",
    "y = tracts_not_null[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute spatial weights for just these observations then row-standardize\n",
    "w_queen = ps.lib.weights.Queen.from_dataframe(tracts_not_null)\n",
    "w_queen.set_transform('R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial lag\n",
    "y_lag = ps.lib.weights.lag_spatial(w_queen, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is a tract's med income similar to those of its neighbors?\n",
    "col_lag = f'{col}_lag'\n",
    "data_lag = pd.DataFrame(data={col:y, col_lag:y_lag}).astype(int)\n",
    "data_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial autocorrelation\n",
    "\n",
    "Spatial autocorrelation is a central question in ESDA. Statistical models typically assume that the observations are independent of each other. This assumption is violated when a variable's value at one location is correlated with its value at nearby locations.\n",
    "\n",
    "Such spatial autocorrelation is common in the real world due to proximity-based spillover effects. For example, a home's value may be a function of its own characteristics and accessibility, but it is also a function of nearby homes' values. In other words, homes near one another tend to have similar home values.\n",
    "\n",
    "  - **positive** spatial autocorrelation: nearby values tend to be more similar (e.g. income, home values, temperature, rainfall)\n",
    "  - **negative** spatial autocorrelation: nearby values tend to be more dissimilar (e.g. fire stations, libraries)\n",
    "\n",
    "Substantive spatial autocorrelation can be explained by social or economic theory that describes a spatial relationship. Nuisance spatial autocorrelation stems from data problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does household income exhibit spatial autocorrelation?\n",
    "# let's find out\n",
    "data_lag.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Moran's I\n",
    "\n",
    "Moran's I measures *global* spatial autocorrelation: do things tend to be near other (dis)similar things. Values > 0 indicate positive spatial autocorrelation, and values < 0 indicate negative spatial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the statistic\n",
    "mi = ps.explore.esda.Moran(data_lag[col], w_queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the I value\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical inference: show the p value\n",
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we generated a large number of maps with the same values but randomly allocated over space, and calculated Moran's I for each of these maps, only 1/1000 of them would display a larger absolute value than the one we computed from the real-world data set. Thus there is a 1/1000 chance of getting the observed value of Moran's I if the spatial distribution of our variable is random. We can conclude that the variable's distribution is statistically significantly postively spatially autocorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# calculate the moran's I of median home values\n",
    "# is it statistically significant? what does it tell you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Moran plots\n",
    "\n",
    "A Moran plot scatter plots the spatially-lagged values (y-axis) vs the original variable's values (x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.regplot(x=col, y=col_lag, data=data_lag, scatter_kws={'s':1, 'color':'gray'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the 95% confidence interval shading and the positive slope. Given the p-value of Moran's I that we calculated earlier, we can conclude that the slope of the line is statistically-significantly different from zero.\n",
    "\n",
    "More useful, however, is a **standardized** Moran plot. Moran's I is the slope of the line in the standardized Moran plot, which makes this all a bit easier to conceptualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the variable's values (i.e., calculate z-scores)\n",
    "y_std = (y - y.mean()) / y.std()\n",
    "y_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial lag of standardized values and save as series with same index\n",
    "y_std_lag = pd.Series(ps.lib.weights.lag_spatial(w_queen, y_std),\n",
    "                      index=y_std.index,\n",
    "                      name=col_lag)\n",
    "y_std_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a simple linear regression model\n",
    "m, b, r, p, se = stats.linregress(x=y_std, y=y_std_lag)\n",
    "print('m={:.4f}, b={:.4f}, r^2={:.4f}, p={:.4f}'.format(m, b, r ** 2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the slope is the same as moran's I, calculated earlier\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized moran's plot\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(x=y_std, y=y_std_lag, s=1, color='gray')\n",
    "\n",
    "# draw quadrants and ignore outliers beyond 3 std devs (99.7% of distribution)\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "\n",
    "# draw a line with moran's I as the slope\n",
    "Xs = pd.Series([-3, 3])\n",
    "Ys = Xs * mi.I\n",
    "line = ax.plot(Xs, Ys, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# visualize a standardized moran's plot of median home values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. LISAs\n",
    "\n",
    "Local Indicators of Spatial Autocorrelation: are there specific areas with high concentrations of (dis)similar values?\n",
    "\n",
    "Moran's I tells us about spatial clustering globally across the data set as a whole. However, it does not tell us where these clusters occur. For that, we need a local measure. Essentially, we will classify the data set's observations into four groups based on the four quadrants of the Moran plot:\n",
    "\n",
    "  1. **HH**: high value near other high values (*hot spots*)\n",
    "  1. **LL**: low value near other low values (*cold spots*)\n",
    "  1. **HL**: high value near low values (*spatial outliers*)\n",
    "  1. **LH**: low value near high values (*spatial outliers*)\n",
    "\n",
    "Let's see what that looks like, visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized moran's plot again, from above, but labeled this time\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(x=y_std, y=y_std_lag, s=1, color='gray')\n",
    "\n",
    "# draw quadrants and ignore outliers beyond 3 std devs\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "\n",
    "# label the quadrants\n",
    "ax.text(1.25, 1.25, 'HH', fontsize=30)\n",
    "ax.text(1.25, -1.75, 'HL', fontsize=30)\n",
    "ax.text(-1.75, 1.25, 'LH', fontsize=30)\n",
    "ax.text(-1.75, -1.75, 'LL', fontsize=30)\n",
    "\n",
    "# draw a line with moran's I as the slope\n",
    "Xs = pd.Series([-3, 3])\n",
    "Ys = Xs * mi.I\n",
    "line = ax.plot(Xs, Ys, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate LISA values using the queen spatial weights\n",
    "lisa = ps.explore.esda.Moran_Local(data_lag[col], w_queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the statistical significance threshold (alpha)\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify whether each observation is significant or not\n",
    "# p-value interpretation same as earlier with moran's I\n",
    "data_lag['significant'] = lisa.p_sim < alpha\n",
    "data_lag['significant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the quadrant each observation belongs to\n",
    "data_lag['quadrant'] = lisa.q\n",
    "data_lag['quadrant'] = data_lag['quadrant'].replace({1:'HH', 2:'LH', 3:'LL', 4:'HL'})\n",
    "data_lag['quadrant'].sort_values().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what have we got in the end?\n",
    "data_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now map the tracts, colored according to their LISA quadrants, to identify clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "# merge original tracts and LISA quadrants data together, plot tracts basemap\n",
    "tracts_lisa = tracts.merge(data_lag, how='left', left_index=True, right_index=True)\n",
    "tracts_lisa.plot(ax=ax, facecolor='#999999', edgecolor='k', linewidth=0.1)\n",
    "\n",
    "# plot each quandrant's tracts (if significant LISA) in a different color\n",
    "quadrant_colors = {'HH':'r', 'LL':'b', 'LH':'skyblue', 'HL':'pink'}\n",
    "for q, c in quadrant_colors.items():\n",
    "    mask = tracts_lisa['significant'] & (tracts_lisa['quadrant']==q)\n",
    "    rows = tracts_lisa.loc[mask]\n",
    "    rows.plot(ax=ax, color=c, edgecolor='k', linewidth=0.1)\n",
    "\n",
    "ax.axis('off')\n",
    "fig.savefig('clusters.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this map?\n",
    "\n",
    "  - Gray tracts have statistically-insignificant LISA value (no local spatial autocorrelation)\n",
    "  - In red we see clusters of tracts with high values surrounded by other high values\n",
    "  - In blue we see clusters of tracts with low values surrounded by other low values\n",
    "  - In pink, we see the first type of spatial outliers: tracts with high values but surrounded by low values\n",
    "  - In light blue we see the other type of spatial outlier: tracts with low values surrounded by other tracts with high values\n",
    "\n",
    "## In-class exercise\n",
    "\n",
    "To practice exploratory spatial analysis, do the following below:\n",
    "\n",
    "  1. Select the tracts in a different CA county\n",
    "  1. Calculate a new spatial weights matrix for this subset\n",
    "  1. Choose a new variable from the data set\n",
    "  1. Calculate its Moran's I\n",
    "  1. Visualize its Moran's plot\n",
    "  1. Calculate and map its LISA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
